name: clusters-aws-reusable-workflow

on:
  workflow_call:
    inputs:
      clusters:
        description: Cluster list
        required: true
        type: string
      terraform_action:
        description: Cluster Terraform action
        required: true
        type: string
      env-variables:
        description: Environment variable(s)
        required: true
        type: string
    secrets:
      CREATE_FLUX_DEPLOY_KEY_GITHUB_TOKEN:
        description: GitHub token used to create Flux deploy key
        required: true

jobs:
  generate-cluster-aws-matrix:
    name: "Generate AWS Cluster matrix"
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Check out repository code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: üí°üî™ Get clusters
        id: set-matrix
        run: |
          set -euxo pipefail

          # Find cluster names matching the given regexp (and exclude `argocd|flux` directory)
          CLUSTERS=$(find clusters -maxdepth 2 -mindepth 2 -type d -regextype "posix-extended" ! -regex '.*/(argocd|flux)' -regex '${{ inputs.clusters }}' -printf "%f\n" | sort)

          echo "*** Export list of clusters"
          # shellcheck disable=SC2001
          echo "${CLUSTERS}" | sed 's@\(.*\)@üîé \1@'
          if [[ -z "${CLUSTERS}" ]] ; then
            echo "üî• No clusters found / selected / ... !!!"
            exit 1
          fi
          echo "::set-output name=matrix::$( echo "${CLUSTERS}" | jq -c -R -s 'split("\n")[:-1]' )"

  cluster-aws-pipeline:
    name: "${{ inputs.terraform_action }} | ${{ matrix.stage }}"
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    # Cancel job after 100 minutes (just in case)
    timeout-minutes: 100
    if: ${{ needs.generate-cluster-aws-matrix.outputs.matrix != '[""]' }}
    needs: generate-cluster-aws-matrix
    env:
      # Terraform variables (https://www.terraform.io/docs/cli/config/environment-variables.html)
      TERRAFORM_VERSION: "1.1.2"
      TF_INPUT: "0"
      TF_CLI_ARGS_apply: "-auto-approve"
      TF_CLI_ARGS_destroy: "-auto-approve"
      TF_IN_AUTOMATION: "true"
      TF_VAR_github_token: ${{ secrets.CREATE_FLUX_DEPLOY_KEY_GITHUB_TOKEN }}
      # https://github.com/helm/helm/releases
      HELM_VERSION: "v3.6.0"
      # https://github.com/kubernetes/kubectl/releases
      KUBECTL_VERSION: "v1.22.8"
      # https://github.com/kubernetes-sigs/kustomize/releases
      KUSTOMIZE_VERSION: "4.4.1"
    # Allow only one execution of terraform per cluster (other executions will wait until first will complete)
    concurrency:
      group: cluster-aws-pipeline-${{ matrix.stage }}
    strategy:
      # Do not cancel matrix jobs if one of them fails
      fail-fast: false
      matrix:
        stage: ${{ fromJSON(needs.generate-cluster-aws-matrix.outputs.matrix) }}

    steps:
      - name: üí°üî™ Check out repository code
        uses: actions/checkout@v3

      - name: üí°üî™ Install necessary tools/packages
        run: |
          set -euxo pipefail
          sudo apt update -qq
          sudo apt-get install -y -qq curl gettext-base git jq unzip > /dev/null

          if ! command -v aws &> /dev/null; then
            curl -sL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "/tmp/awscliv2.zip"
            unzip -q -o /tmp/awscliv2.zip -d /tmp/
            sudo /tmp/aws/install
          else
            aws --version
          fi

          if ! command -v kustomize &> /dev/null; then
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | sudo bash -s ${{ env.KUSTOMIZE_VERSION }} /usr/local/bin/
          else
            kustomize version
          fi

      - name: Setup kubectl
        uses: azure/setup-kubectl@v2.1
        with:
          version: ${{ env.KUBECTL_VERSION }}

      - name: Setup helm
        uses: azure/setup-helm@v1
        with:
          version: ${{ env.HELM_VERSION }}

      - name: üí°üî™ Get and display environment variables
        run: |
          set -euxo pipefail

          # Funcion to extract variables from "${CLUSTER_PATH}/cluster-variables.tfvars" "${CLUSTER_PATH}/../group-variables.tfvars"
          # Example: get_variable_from_group_cluster_tfvars /cluster/mycluster my_lowercase_variable_name
          get_variable_from_group_cluster_tfvars () {
            local CLUSTER_PATH="$1" TF_CODE_VARIABLE="$2" VARIABLE_HELPER
            if grep -q "${TF_CODE_VARIABLE}" "${CLUSTER_PATH}/cluster-variables.tfvars" ; then
              VARIABLE_HELPER=$(awk -F \" "/^${TF_CODE_VARIABLE}/ { print \$2 }" "${CLUSTER_PATH}/cluster-variables.tfvars")
            else
              VARIABLE_HELPER=$(awk -F \" "/^${TF_CODE_VARIABLE}/ { print \$2 }" "${CLUSTER_PATH}/../group-variables.tfvars")
            fi
            echo -e "\nüí° Variable: \"${TF_CODE_VARIABLE^^}\" = \"${VARIABLE_HELPER}\""
            echo "${TF_CODE_VARIABLE^^}=${VARIABLE_HELPER}" >> "${GITHUB_ENV}"
            export "${TF_CODE_VARIABLE^^}=${VARIABLE_HELPER}"
            echo "::set-output name=${TF_CODE_VARIABLE^^}::${VARIABLE_HELPER}"
          }

          echo -e "üéâ The job was automatically triggered by a \"${{ github.event_name }}\" event."
          echo -e "üí° The name of your branch is ${{ github.ref }}"
          echo -e "üíä Action: ${{ inputs.terraform_action }}"

          # Find cluster path based on cluster FQDN (matrix.stage)
          CLUSTER_PATH=$(find clusters -maxdepth 2 -mindepth 2 -type d -regextype "posix-extended" ! -regex '.*/(argocd|flux)' -regex '.*${{ matrix.stage }}.*')
          echo -e "\nüçè Cluster path: ${CLUSTER_PATH}"
          echo "CLUSTER_PATH=${CLUSTER_PATH}" >> "${GITHUB_ENV}"

          get_variable_from_group_cluster_tfvars "${CLUSTER_PATH}" "aws_default_region"
          get_variable_from_group_cluster_tfvars "${CLUSTER_PATH}" "aws_assume_role"
          get_variable_from_group_cluster_tfvars "${CLUSTER_PATH}" "cluster_fqdn"
          echo "CLUSTER_NAME=${CLUSTER_FQDN%%.*}" | tee -a "${GITHUB_ENV}"
          get_variable_from_group_cluster_tfvars "${CLUSTER_PATH}" "terraform_code_dir"

          echo -e "\nüçè Set pre-defined environment variables (if any)"
          # shellcheck disable=SC2043
          for ENV_VARIABLE in ${{ inputs.env-variables }} ; do
            echo "${ENV_VARIABLE}" | tee -a "${GITHUB_ENV}"
          done

      - name: üí° Amazon EKS cluster access (kubeconfig)
        if: ${{ inputs.terraform_action == 'apply' }}
        run: |
          cat << EOF
          export KUBECONFIG="/tmp/kubeconfig-${CLUSTER_NAME}.conf"
          eval "\$(aws sts assume-role --role-arn "${AWS_ASSUME_ROLE}" --role-session-name "$USER@\$(hostname -f)-k8s-tf-eks-gitops-\$(date +%s)" --duration-seconds 36000 | jq -r '.Credentials | "export AWS_ACCESS_KEY_ID=\(.AccessKeyId)\nexport AWS_SECRET_ACCESS_KEY=\(.SecretAccessKey)\nexport AWS_SESSION_TOKEN=\(.SessionToken)\n"')"
          aws eks update-kubeconfig --region "${AWS_DEFAULT_REGION}" --name "${CLUSTER_NAME}" --kubeconfig "\$KUBECONFIG"
          EOF

      - name: üí°üî™ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          role-to-assume: ${{ env.AWS_ASSUME_ROLE }}
          role-session-name: GitHubOidcFederatedRole
          aws-region: ${{ env.AWS_DEFAULT_REGION }}

      - name: üí° Create S3 bucket + DynamoDB for Terraform if needed
        if: ${{ ! startsWith(inputs.terraform_action, 'destroy') }}
        run: |
          set -euxo pipefail
          aws sts get-caller-identity
          echo -e "\n***üí° Check if S3 bucket exists (may show 404 error - ignore)"
          if ! aws s3api head-bucket --bucket "${CLUSTER_FQDN}" ; then
            echo -e "\n***üí° Creating S3 bucket for Terraform using CloudFormation"
            aws cloudformation deploy \
              --parameter-overrides "ClusterFQDN=${CLUSTER_FQDN}" \
              --stack-name "${CLUSTER_FQDN//./-}-s3-dynamodb-tfstate" --template-file "./cloudformation/s3-dynamodb-tfstate.yaml"
          else
            echo -e "\n***üí° S3 bucket for Terraform - \"${CLUSTER_FQDN}\" already exists...\n"
          fi

      - name: üí°üî™ Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      # Terraform needs node command: https://github.com/hashicorp/setup-terraform/issues/84
      - uses: actions/setup-node@v3
        with:
          node-version: '16'

      - name: üí°üî™ Terraform init
        run: |
          set -euxo pipefail
          terraform -chdir="${TERRAFORM_CODE_DIR}" init \
            -backend-config="bucket=${CLUSTER_FQDN}" \
            -backend-config="key=terraform-${CLUSTER_FQDN}.tfstate" \
            -backend-config="region=${AWS_DEFAULT_REGION}" \
            -backend-config="dynamodb_table=${CLUSTER_FQDN}"

      # Some internal apps creating AWS objectls like (Loadbalancers, Route53)
      # which then prevents terrafrom to "cleanly" remove all it's objects.
      # Therefore I need to remove them in k8s which will cause deleting them
      # from AWS
      - name: üî™ Delete k8s objects Loadbalancer, external-dns
        if: ${{ startsWith(inputs.terraform_action, 'destroy') }}
        run: |
          set -euxo pipefail

          # if [[ "$(aws eks list-clusters)" =~ \"${CLUSTER_NAME}\" ]] ; then
          #   export KUBECONFIG="/tmp/kubeconfig-${CLUSTER_NAME}.conf"
          #   aws eks update-kubeconfig --name "${CLUSTER_NAME}" --kubeconfig "${KUBECONFIG}"

          #   # !!! TODO - stop argo sync, delete all services which are type=Loadbalancers + uninstall external-dns
          #   kubectl get nodes
          #   kubectl delete deployments -A -l app.kubernetes.io/name=external-dns
          #   rm "/tmp/kubeconfig-${CLUSTER_NAME}.conf"
          # fi

          # Remove Network ELBs
          for NETWORK_ELB_ARN in $(aws elbv2 describe-load-balancers --query "LoadBalancers[].LoadBalancerArn" --output=text) ; do
            if [[ "$(aws elbv2 describe-tags --resource-arns "${NETWORK_ELB_ARN}" --query "TagDescriptions[].Tags[?Key == \`kubernetes.io/cluster/${CLUSTER_NAME}\`]" --output text)" =~ ${CLUSTER_NAME} ]]; then
              echo "üíä Deleting Network ELB: ${NETWORK_ELB_ARN}"
              aws elbv2 delete-load-balancer --load-balancer-arn "${NETWORK_ELB_ARN}"
            fi
          done

          # Remove Classic ELBs
          for CLASSIC_ELB_NAME in $(aws elb describe-load-balancers --query "LoadBalancerDescriptions[].LoadBalancerName" --output=text) ; do
            if [[ "$(aws elb describe-tags --load-balancer-names "${CLASSIC_ELB_NAME}" --query "TagDescriptions[].Tags[?Key == \`kubernetes.io/cluster/${CLUSTER_NAME}\`]" --output text)" =~ ${CLUSTER_NAME} ]]; then
              echo "üíä Deleting Classic ELB: ${CLASSIC_ELB_NAME}"
              aws elb delete-load-balancer --load-balancer-name "${CLASSIC_ELB_NAME}"
            fi
          done

          CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name==\`${CLUSTER_FQDN}.\`].Id" --output text)
          if [[ -n "${CLUSTER_FQDN_ZONE_ID}" ]]; then
            aws route53 list-resource-record-sets --hosted-zone-id "${CLUSTER_FQDN_ZONE_ID}" | jq -c '.ResourceRecordSets[] | select (.Type != "SOA" and .Type != "NS")' |
            while read -r RESOURCERECORDSET; do
              aws route53 change-resource-record-sets \
                --hosted-zone-id "${CLUSTER_FQDN_ZONE_ID}" \
                --change-batch '{"Changes":[{"Action":"DELETE","ResourceRecordSet": '"${RESOURCERECORDSET}"' }]}' \
                --output text --query 'ChangeInfo.Id'
            done
          fi

      - name: üí°üî™ Terraform action
        run: |
          set -euxo pipefail
          terraform -chdir="${TERRAFORM_CODE_DIR}" ${{ inputs.terraform_action }} \
            -var-file="${PWD}/${CLUSTER_PATH}/../../main-variables.tfvars" \
            -var-file="${PWD}/${CLUSTER_PATH}/../group-variables.tfvars" \
            -var-file="${PWD}/${CLUSTER_PATH}/cluster-variables.tfvars"

      - name: üî™ Delete Volumes / Snapshost created by k8s cluster
        if: ${{ startsWith(inputs.terraform_action, 'destroy') }}
        run: |
          set -euxo pipefail

          VOLUMES=$(aws ec2 describe-volumes --filter "Name=tag:Cluster,Values=${CLUSTER_FQDN}" --query 'Volumes[].VolumeId' --output text) && \
          for VOLUME in ${VOLUMES}; do
            echo "Removing Volume: ${VOLUME}"
            aws ec2 delete-volume --volume-id "${VOLUME}"
          done

          SNAPSHOTS=$(aws ec2 describe-snapshots --filter "Name=tag:Cluster,Values=${CLUSTER_FQDN}" --query 'Snapshots[].SnapshotId' --output text) && \
          for SNAPSHOT in ${SNAPSHOTS}; do
            echo "Removing Snapshot: ${SNAPSHOT}"
            aws ec2 delete-snapshot --snapshot-id "${SNAPSHOT}"
          done

      - name: üî™ Delete S3 bucket + DynamoDB used by Terraform
        if: ${{ startsWith(inputs.terraform_action, 'destroy') }}
        run: |
          set -euxo pipefail

          S3_OBJECTS=$(aws s3api list-object-versions --bucket "${CLUSTER_FQDN}" --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}')

          if [[ ! "${S3_OBJECTS}" =~ "\"Objects\": null" ]]; then
            aws s3api delete-objects --bucket "${CLUSTER_FQDN}" \
              --delete "${S3_OBJECTS}" \
              --output=json | jq
          fi

          aws cloudformation delete-stack --stack-name "${CLUSTER_FQDN//./-}-s3-dynamodb-tfstate"
          aws cloudformation wait stack-delete-complete --stack-name "${CLUSTER_FQDN//./-}-s3-dynamodb-tfstate"
